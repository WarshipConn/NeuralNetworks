{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2ee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#test import to make sure that the environment works\n",
    "import torch\n",
    "\n",
    "layer = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d36a2",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827b78d",
   "metadata": {},
   "source": [
    "Outline Very Basic:\n",
    "\n",
    "#Idea behind Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "While machines are potent in processing defined algorithms, it has trouble in more abstract problems such as differentiating between a photo of a dog from a cat. To humans, this task may be trivial. However, humans themselves are also unable to explain their thought process, as their brain processes the information in something akin to a black box, unsure of the exact algorithms underneath. \n",
    "\n",
    "\n",
    "\n",
    "**The broadest idea of machine learning is that there are intrinsic patterns in data. By matching and gathering a large amount input and output pairs, it may be possible to find the function or formula which converts the input into the output.**\n",
    "\n",
    "The rest of this paper would discuss the more practical concepts in implementing simpler neural network models.\n",
    "\n",
    "\n",
    "#Neurons and Linear Layers:\n",
    "\n",
    "The idea behind a neuron is that it will take an input signal and return a modified output signal based on its weight attribute. For example, a neuron with a large weight would amplify the input signal into a larger output signal, and vice versa.\n",
    "\n",
    "The main idea for almost all neural networks are its layers. Each layer would have a certain amount of \"neurons\" or \"nodes\", which would each hold a weight value. By assigning different weights to each neuron in the layers, the input signals will get amplified or diminished in its corresponding areas.\n",
    "\n",
    "Besides the, a non-linear function is . Common examples include sigmoid and tanh.\n",
    "\n",
    "Practically, all weights, inputs, and biases are represented as matrices or tensors, both of which are a common way to group large amount of numbers. This allows for the ease of processing large amount of calculations which neural networks need. Alternatively, they are similar to an array of Nth dimension, depending on the requirements.\n",
    "\n",
    "After summarizing everything, here are the general formula of a single neural network layer.\n",
    "\n",
    "General formula:\n",
    "\n",
    "y = f(x1w1 + x2w2 + x3w3 + ... + xnwn + b)\n",
    "\n",
    "Formula in matrix form:\n",
    "\n",
    "y = f(X * W + b)\n",
    "\n",
    "where X is the input matrix, W is the matrix containing the weights, b is the bias term, and f is the non-linear function.\n",
    "\n",
    "\n",
    "#Forward Pass and Backpropagation:\n",
    "\n",
    "The formula described previously is the definition of a forward pass, which means putting inputs into a neural network and obtaining an output from it.\n",
    "\n",
    "Backpropagation, on the other hand, uses an output and passes it back through the network to update its weights and biases. \n",
    "\n",
    "\n",
    "Project result:\n",
    "trained XOR\n",
    "\n",
    "\n",
    "Neural Network training:\n",
    "Problems: overfitting, underfitting\n",
    "Techniques: Learning rate adjustment (optimizers), momentum(adagrad), dropout layers\n",
    "\n",
    "\n",
    "Convolutional Layers:\n",
    "Image processing, identify edges\n",
    "simplify into smaller tensor (embedded vectors?)\n",
    "\n",
    "Project result:\n",
    "MNIST Reader\n",
    "ResNet18 with CIFAR-10\n",
    "ViT with CIFAR-10\n",
    "\n",
    "\n",
    "Generative Programs:\n",
    "GAN\n",
    "generator vs discriminator\n",
    "\n",
    "DDRM diffusion\n",
    "image to noise and vise versa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
