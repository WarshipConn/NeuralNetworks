{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2ee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#test import to make sure that the environment works\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "layer = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d36a2",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827b78d",
   "metadata": {},
   "source": [
    "Outline Very Basic:\n",
    "\n",
    "#The Main Idea behind Machine Learning\n",
    "\n",
    "It is well known that machines are potent in processing defined algorithms with its combination of speed, memory, and accuracy. Once a human defined an algorithm, or a series of steps, for the computer to follow, it can do so faster and better than any other human.\n",
    "\n",
    "However, machines themselves are unable to tackle the more abstract problems such as differentiating between a photo of a dog from a cat. \n",
    "\n",
    "To humans, this task may be trivial. However, humans themselves are also unable to clearly explain their thought process for separating dogs and cats in a concise way. They may suggest tips such as looking at its ears or tail, but this is another ambiguous question in itself, especially to a computer which percieves images not by its greater pattern, but each individual pixel and its color values. For humans, their brains are able to intuitively process the information in something akin to a black box, unsure of the exact algorithms underneath. Since humans are also unable to create a concise algorithm for classification, they cannot write code for a machine to follow in order to accomplish the same task.\n",
    "\n",
    "So, does this mean that humans were born with an innate ability to differentiate between dogs and cats? There are no strong evidence supporting this argument, so the leading theory is that humans develop their classification abilities later on, probably by observing an uncountable amount of dogs and cats throughout their lives. This implies that the classification process can be learnt, most likely by identifying groups of hidden patterns that gives deeper insight than just the raw data itself.\n",
    "\n",
    "**The broadest idea of machine learning is that there are intrinsic patterns in data. By matching and gathering a large amount input and output pairs, it may be possible to find the function or formula which converts an input into the desired corresponding output.**\n",
    "\n",
    "The rest of this paper would discuss the more practical concepts in implementing simpler neural network models.\n",
    "\n",
    "#Embedding: Representing Information:\n",
    "\n",
    "Before making. This is most commonly done through vectors, matrices, and tensors. These are essencially an array or list of a certain dimension. This process is known as embedding.\n",
    "\n",
    "#Neurons and Linear Layers:\n",
    "\n",
    "The idea behind a neuron is that it is the smallest possible component in a larger neural network, just like a human's neuron to their brain. While biology and chemistry powers a human neuron, a machine's neuron is defined by math.\n",
    "\n",
    "In mathamatics, multiplication is the most common way to alter a value's size by its proportion. For instance, multiplying X by 0.5 yields X/2, something half as large in magnitude, while multiplying X by 2 yields 2X, something twice as large in magnitude. This is a useful way to amplify or diminish a value's magnitude without changing its inherent composition (attributes such as its prime factors). Another simple way to manipulate values is addition. This operation can shift a value along the number line, or alternatively a vector along a certain axis. Although addition also affects the size of a value, it will disturb said value's composition. Overall, it is best to think of multiplication as adjusting a value's size, while addition acts as an offset.\n",
    "\n",
    "These mathmatical ideas also applies to the field of machine learning, which comes in the form of a neuron. The neuron would be akin to a function, with two inherent adjustable properties known as the weight and the bias. The idea behind a neuron is that it will take an input signal, multiply it with its weight attribute, add the product with its bias attribute, and return the final sum as its modified output signal. For example, a neuron with a large weight would amplify the input signal into a larger output signal, and vice versa.\n",
    "\n",
    "Here is the simple formula for a single neuron:\n",
    "\n",
    "*y = wx + b*\n",
    "*(where w is the weight, x is the input, and b is the bias value)*\n",
    "\n",
    "\n",
    "Neurons are then organized into layers.\n",
    "\n",
    "The main idea for almost all neural networks are its layers. Each layer would have a certain amount of \"neurons\" or \"nodes\", which would each hold a weight value. By assigning different weights to each neuron in the layers, the input signals will get amplified or diminished in its corresponding areas.\n",
    "\n",
    "Practically, all weights, inputs, and biases are represented as matrices or tensors, both of which are a common way to group large amount of numbers. This allows for the ease of processing large amount of calculations which neural networks need. Alternatively, they are similar to an array of Nth dimension, depending on the requirements.\n",
    "\n",
    "Besides the, a non-linear function is . Common examples include sigmoid and tanh.\n",
    "\n",
    "Here is a human-friendly example of a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33cfb81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999993086"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inputs\n",
    "X = np.array([2, 3, 5])\n",
    "\n",
    "#Neural Layer Properties\n",
    "W = np.array([1, 2, 4])\n",
    "B = 0\n",
    "\n",
    "#Non-linear function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class linear_neuron_layer:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sigmoid(sum(x*self.w) + self.b)\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "neuron = linear_neuron_layer(W, B)\n",
    "\n",
    "neuron.forward(X)\n",
    "#y = sigmoid(28)\n",
    "#y = 0.9999999999993086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac099501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999993086"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Inputs\n",
    "X = np.array([2, 3, 5])\n",
    "\n",
    "#Neural Layer Properties\n",
    "W = np.array([1, 2, 4])\n",
    "B = 0\n",
    "\n",
    "#Functions\n",
    "def sigmoid(x): #Sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def error(y, t): #L2 Norm\n",
    "    return 0.5 * np.power((t-y), 2)\n",
    "\n",
    "def errorDerivative(y, t):\n",
    "    return -(t-y)\n",
    "\n",
    "class linear_neuron_layer:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sigmoid(sum(x*self.w) + self.b)\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "    \n",
    "    def updateWeights(self, g, u):\n",
    "        if len(self.w) == len(g):\n",
    "            self.w = (self.w + g) * u\n",
    "\n",
    "neuron = linear_neuron_layer(W, B)\n",
    "\n",
    "neuron.forward(X)\n",
    "#y = sigmoid(28)\n",
    "#y = 0.9999999999993086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d3f5f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------XOR Output\n",
      "0.5\n",
      "0.4357839017110881\n",
      "0.2245687444884695\n",
      "0.1827944569485668\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "bigErr = 100000\n",
    "\n",
    "#Helpers\n",
    "def sigmoid(x): #Sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def error(y, t): #L2 Norm\n",
    "    return 0.5 * np.power((t-y), 2)\n",
    "\n",
    "def errorDerivative(y, t):\n",
    "    return -(t-y)\n",
    "\n",
    "class NeuronLayerSingle:\n",
    "    def __init__(self, w, b, f):\n",
    "        self.w = w #weight list\n",
    "        self.b = b #bias value\n",
    "        self.f = f #non-linear function\n",
    "    \n",
    "    #functions\n",
    "    def updateWeights(self, g, u):\n",
    "        if len(self.w) == len(g):\n",
    "            self.w = (self.w + g) * u\n",
    "\n",
    "    def getInputSize(self):\n",
    "        return len(self.w)\n",
    "\n",
    "    def getOutputRaw(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sum(x*self.w) + self.b\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "    def getOutput(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return self.f(self.getOutputRaw(x))\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "\n",
    "class TestModelSingle: #Single Layer Only\n",
    "    def __init__(self, inputSize):\n",
    "\n",
    "        mag = 1 / np.sqrt(inputSize) #shallow weight initialization\n",
    "\n",
    "        self.layer = NeuronLayerSingle(np.array([(np.random.rand() * mag * 2 - mag) for i in range(inputSize)]), 0., sigmoid) #Neuron layer, init values [-1/sqrt(x), 1/sqrt(x)]\n",
    "        self.output = None\n",
    "\n",
    "        self.err = bigErr\n",
    "\n",
    "    def run(self, x):\n",
    "        self.output = self.layer.getOutput(x)\n",
    "        return self.output\n",
    "    \n",
    "    def trainOnce(self, x, t, u):\n",
    "        y = self.run(x)\n",
    "        s = self.layer.getOutputRaw(x)\n",
    "        err = errorDerivative(y, t)\n",
    "        g = err * sigmoidDerivative(s) * x\n",
    "\n",
    "        self.layer.updateWeights(g, u)\n",
    "\n",
    "def actualXOR(x):\n",
    "    a = x[0]\n",
    "    b = x[1]\n",
    "    if a == b:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "trainedXOR = TestModelSingle(2)\n",
    "\n",
    "xorOriginalWeight = trainedXOR.layer.w\n",
    "\n",
    "for i in range(250): #arbitrary amount of training\n",
    "    randomInput = np.array([int(np.random.rand() * 2) for i in range(2)])\n",
    "\n",
    "    trainedXOR.trainOnce(randomInput, actualXOR(randomInput), 0.97)\n",
    "\n",
    "#Note: u values were cherry picked for better results\n",
    "#Another Note: Bias is always set to 0, so when input is [0, 0], the result would always be 0.5, since weights won't affect output\n",
    "\n",
    "print(\"-------------XOR Output\")\n",
    "\n",
    "print(trainedXOR.run(np.array([0,0]))) #0, 0.5\n",
    "print(trainedXOR.run(np.array([0,1]))) #1, 0.10408986683883949\n",
    "print(trainedXOR.run(np.array([1,0]))) #1, 0.2770619324755566\n",
    "print(trainedXOR.run(np.array([1,1]))) #0, 0.042628520033774785\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c938e",
   "metadata": {},
   "source": [
    "Besides the, a non-linear function is . Common examples include sigmoid and tanh.\n",
    "\n",
    "\n",
    "\n",
    "After summarizing everything, here are the general formula of a single neural network layer.\n",
    "\n",
    "General formula:\n",
    "\n",
    "y = f(x1w1 + x2w2 + x3w3 + ... + xnwn + b)\n",
    "\n",
    "Formula in matrix form:\n",
    "\n",
    "y = f(X * W + b)\n",
    "\n",
    "where X is the input matrix, W is the matrix containing the weights, b is the bias term, and f is the non-linear function.\n",
    "\n",
    "\n",
    "#Forward Pass and Backpropagation:\n",
    "\n",
    "The formula described previously is the definition of a forward pass, which means putting inputs into a neural network and obtaining an output from it.\n",
    "\n",
    "Backpropagation, on the other hand, uses an output and passes it back through the network to update its weights and biases. \n",
    "\n",
    "Project result:\n",
    "trained XOR\n",
    "\n",
    "\n",
    "#Common Pitfalls and Solutions\n",
    "\n",
    "Neural Network training:\n",
    "Problems: overfitting, underfitting\n",
    "Techniques: Learning rate adjustment (optimizers), momentum(adagrad), dropout layers\n",
    "\n",
    "\n",
    "\n",
    "#BELOW ARE UNFINISHED\n",
    "\n",
    "Convolutional Layers:\n",
    "Image processing, identify edges\n",
    "simplify into smaller tensor (embedded vectors?)\n",
    "\n",
    "Project result:\n",
    "MNIST Reader\n",
    "ResNet18 with CIFAR-10\n",
    "ViT with CIFAR-10\n",
    "\n",
    "\n",
    "Generative Programs:\n",
    "GAN\n",
    "generator vs discriminator\n",
    "\n",
    "DDRM diffusion\n",
    "image to noise and vise versa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
