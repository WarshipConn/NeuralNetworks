{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2ee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#test import to make sure that the environment works\n",
    "import torch\n",
    "\n",
    "layer = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d36a2",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827b78d",
   "metadata": {},
   "source": [
    "Outline Very Basic:\n",
    "\n",
    "#The Main Idea behind Machine Learning\n",
    "\n",
    "It is well known that machines are potent in processing defined algorithms. Its combination of speed, memory, and accuracy . Once a human defined an algorithm, or a series of steps, for the computer to follow, it can do so faster and better than any other human.\n",
    "\n",
    "However, when it comes to more abstract problems such as differentiating between a photo of a dog from a cat. \n",
    "\n",
    "To humans, this task may be trivial. However, humans themselves are also unable to clearly explain their thought process for separating dogs and cats in a concise way. They may suggest tips such as looking at its ears or tail, but this is another ambiguous question in itself, especially to a computer which percieves images not by its greater pattern, but each individual pixel and its color values. For humans, their brains are able to intuitively process the information in something akin to a black box, unsure of the exact algorithms underneath. \n",
    "\n",
    "So, does this mean that humans were born with an innate ability to differentiate between dogs and cats? There are no strong evidence supporting this argument, so the leading theory is that humans develop their classification abilities later on, probably by observing an uncountable amount of dogs and cats throughout their lives. This implies that the classification process can be learnt, most likely by identifying groups of hidden patterns that gives deeper insight than just the raw data itself.\n",
    "\n",
    "**The broadest idea of machine learning is that there are intrinsic patterns in data. By matching and gathering a large amount input and output pairs, it may be possible to find the function or formula which converts an input into the desired corresponding output.**\n",
    "\n",
    "The rest of this paper would discuss the more practical concepts in implementing simpler neural network models.\n",
    "\n",
    "#Embedding: Representing Information:\n",
    "\n",
    "Before making. This is most commonly done through vectors, matrices, and tensors. These are essencially an array or list of a certain dimension. This process is known as embedding.\n",
    "\n",
    "#Neurons and Linear Layers:\n",
    "\n",
    "The idea behind a neuron is that it will take an input signal and return a modified output signal based on its weight attribute. For example, a neuron with a large weight would amplify the input signal into a larger output signal, and vice versa.\n",
    "\n",
    "In mathamatics, multiplication is the most common way to alter a value's size by its proportion. For instance, multiplying X by 0.5 yields X/2, something half as large in magnitude, while multiplying X by 2 yields 2X, something twice as large in magnitude. This is a useful way to amplify or diminish a value's magnitude without changing its inherent composition (attributes such as its prime factors). Another simple way to manipulate values is addition. This operation can shift a value along the number line, or alternatively a vector along a certain axis. Although addition also affects the size of a value, it will disturb said value's composition. Overall, it is best to think of multiplication as adjusting a value's size, while addition acts as an offset.\n",
    "\n",
    "These mathmatical ideas also applies to the field of machine learning, which comes in the form of a neuron. The neuron would be akin to a function, with two inherent adjustable properties known as the weight and the bias. The idea behind a neuron is that it will take an input signal, multiply it with its weight attribute, add the product with its bias attribute, and return the final sum as its modified output signal. For example, a neuron with a large weight would amplify the input signal into a larger output signal, and vice versa.\n",
    "\n",
    "Here is the simple formula for a single neuron:\n",
    "\n",
    "*y = wx + b*\n",
    "*(where w is the weight, x is the input, and b is the bias value)*\n",
    "\n",
    "\n",
    "Neurons are then organized into layers.\n",
    "\n",
    "The main idea for almost all neural networks are its layers. Each layer would have a certain amount of \"neurons\" or \"nodes\", which would each hold a weight value. By assigning different weights to each neuron in the layers, the input signals will get amplified or diminished in its corresponding areas.\n",
    "\n",
    "\n",
    "\n",
    "Besides the, a non-linear function is . Common examples include sigmoid and tanh.\n",
    "\n",
    "Practically, all weights, inputs, and biases are represented as matrices or tensors, both of which are a common way to group large amount of numbers. This allows for the ease of processing large amount of calculations which neural networks need. Alternatively, they are similar to an array of Nth dimension, depending on the requirements.\n",
    "\n",
    "After summarizing everything, here are the general formula of a single neural network layer.\n",
    "\n",
    "General formula:\n",
    "\n",
    "y = f(x1w1 + x2w2 + x3w3 + ... + xnwn + b)\n",
    "\n",
    "Formula in matrix form:\n",
    "\n",
    "y = f(X * W + b)\n",
    "\n",
    "where X is the input matrix, W is the matrix containing the weights, b is the bias term, and f is the non-linear function.\n",
    "\n",
    "\n",
    "#Forward Pass and Backpropagation:\n",
    "\n",
    "The formula described previously is the definition of a forward pass, which means putting inputs into a neural network and obtaining an output from it.\n",
    "\n",
    "Backpropagation, on the other hand, uses an output and passes it back through the network to update its weights and biases. \n",
    "\n",
    "\n",
    "Project result:\n",
    "trained XOR\n",
    "\n",
    "\n",
    "#Common Pitfalls and Solutions\n",
    "\n",
    "Neural Network training:\n",
    "Problems: overfitting, underfitting\n",
    "Techniques: Learning rate adjustment (optimizers), momentum(adagrad), dropout layers\n",
    "\n",
    "\n",
    "\n",
    "#BELOW ARE UNFINISHED\n",
    "\n",
    "Convolutional Layers:\n",
    "Image processing, identify edges\n",
    "simplify into smaller tensor (embedded vectors?)\n",
    "\n",
    "Project result:\n",
    "MNIST Reader\n",
    "ResNet18 with CIFAR-10\n",
    "ViT with CIFAR-10\n",
    "\n",
    "\n",
    "Generative Programs:\n",
    "GAN\n",
    "generator vs discriminator\n",
    "\n",
    "DDRM diffusion\n",
    "image to noise and vise versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c938e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
