{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2ee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#test import to make sure that the environment works\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "layer = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d36a2",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827b78d",
   "metadata": {},
   "source": [
    "Outline Very Basic:\n",
    "\n",
    "#The Main Idea behind Machine Learning\n",
    "\n",
    "It is well known that machines are potent in processing defined algorithms with its combination of speed, memory, and accuracy. Once a human defined an algorithm, or a series of steps, for the computer to follow, it can do so faster and better than any other human.\n",
    "\n",
    "However, machines themselves are unable to tackle the more abstract problems such as differentiating a photo of a dog from a cat. \n",
    "\n",
    "To humans, this task may be trivial. However, humans themselves are unable to clearly explain their thought process for separating dogs and cats in a concise way. They may suggest tips such as looking at its ears or tail, but this is another ambiguous question in itself, especially to a computer which percieves images not by its greater pattern, but by each individual pixel and its color values. For humans, their brains act like a black box: being able to intuitively process the information accurately but unsure of the exact algorithms underneath. As humans are unable to create a concise algorithm for such intuitive tasks, they cannot write code for a machine to follow in order to accomplish the same task.\n",
    "\n",
    "So, how do humans do it? Does this mean that humans were born with an innate ability to differentiate between dogs and cats? There are no strong evidence supporting this argument, so the leading theory is that humans develop their classification abilities later on, probably by observing an uncountable amount of dogs and cats throughout their lives. This implies that the classification process can be learnt, most likely by identifying groups of hidden patterns that gives deeper insight than just the raw data itself.\n",
    "\n",
    "**The broadest idea of machine learning is that there are intrinsic patterns in data. By matching and gathering a large amount input and output pairs, it may be possible to find the function or formula which converts an input into the desired corresponding output.**\n",
    "\n",
    "The rest of this paper would discuss the more practical concepts in implementing simpler neural network models.\n",
    "\n",
    "#Embedding Vectors and Representing Information (WIP):\n",
    "\n",
    "Before making a neural network, there needs to be a quantitative way of representing the information mathamatically. This is most commonly done through vectors, matrices, and tensors. These are essencially an array or list of a certain dimension. The process of converting information from one form to a vector space is known as embedding. The general idea is to map objects in the vector space based on their properties, so that more similiar items have a smaller difference. \n",
    "\n",
    "Usually, each dimension or direction in the vector space would represent a certain trait or attribute. For instance, in a good embedding of English words, the difference between vectors representing man and woman should be very similar to the difference of vectors of king and queen, boy and girl, father and mother, and so on. However in practice, larger neural networks may organize their data in another unknown method in their training.\n",
    "\n",
    "#Neurons and Linear Layers:\n",
    "\n",
    "The idea behind a neuron is that it is the smallest possible component in a larger neural network, just like a human's neuron to their brain. While biology and chemistry powers a human neuron, a machine's neuron is defined by math.\n",
    "\n",
    "In mathamatics, multiplication is the most common way to alter a value's size by its proportion. For instance, multiplying X by 0.5 yields X/2, something half as large in magnitude. Meanwhile, multiplying X by 2 yields 2X, something twice as large in magnitude. This is a useful way to amplify or diminish a value's magnitude without changing its inherent composition (attributes such as its prime factors, which may carry information). For instance, if you multiply 15 by 2 to get 30, it still contain the prime factors 3 and 5 afterwards. Another simple way to manipulate values is addition. This operation can shift a value along the number line, or alternatively a vector along a certain axis. Although addition also affects the size of a value, it will disturb said value's composition. For instance, unlike previously with multiplication, if you add 15 by 2, the result 17 no longer contains the prime factor 3 and 5. Overall, it is best to think of multiplication as adjusting a value's size, while addition acts as an offset.\n",
    "\n",
    "These mathmatical ideas also applies to the field of machine learning, which comes in the form of a neuron. Instead of just being a numerical value, the input to a neuron is assumed to represent embedded information in some way unknown to us. The neuron is then able to amplify and offset the input into the final output, which is akin to adjust its significance or value. Practically, the neuron accomplish this by being a function, with two inherent adjustable properties known as the weight and the bias. The neuron will take the input signal, multiply it with its weight attribute, add the product with its bias attribute, and return the final sum as its modified output signal. For example, a neuron with a large weight would amplify the input signal into a larger output signal, and vice versa.\n",
    "\n",
    "Here is a simple formula for a single neuron that incorporated the concepts from above:\n",
    "\n",
    "*y = wx + b*\n",
    "*(where w is the weight, x is the input, and b is the bias value)*\n",
    "\n",
    "Besides the core components of weights and biases, a non-linear function is also needed to help neurons with its expressiveness. \n",
    "\n",
    "Taking a look at the current model (y = wx + b), it is a linear function. However, not all input-output pairs can be represented by a linear model, a famous example of which is the xor logic gate. \n",
    "\n",
    "\n",
    "Common examples include sigmoid and tanh.\n",
    "\n",
    "\n",
    "Neurons are then organized into layers.\n",
    "\n",
    "The main idea for almost all neural networks are its layers. Each layer would have a certain amount of \"neurons\" or \"nodes\", which would each hold a weight value. By assigning different weights to each neuron in the layers, the input signals will get amplified or diminished in its corresponding areas.\n",
    "\n",
    "Practically, all weights, inputs, and biases are represented as matrices or tensors, both of which are a common way to group large amount of numbers. This allows for the ease of processing large amount of calculations which neural networks need. Alternatively, they are similar to an array of Nth dimension, depending on the requirements.\n",
    "\n",
    "\n",
    "(Add more about non-linear functions)\n",
    "Besides the, a non-linear function is . Common examples include sigmoid and tanh.\n",
    "\n",
    "Here is a human-friendly example of a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33cfb81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999993086"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inputs\n",
    "X = np.array([2, 3, 5])\n",
    "\n",
    "#Neural Layer Properties (Given)\n",
    "W = np.array([1, 2, 4])\n",
    "B = 0\n",
    "\n",
    "#Non-linear function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class linear_neuron_layer:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sigmoid(sum(x*self.w) + self.b)\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "neuron = linear_neuron_layer(W, B)\n",
    "\n",
    "neuron.forward(X)\n",
    "#y = sigmoid(28)\n",
    "#y = 0.9999999999993086"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c938e",
   "metadata": {},
   "source": [
    "After summarizing everything, here are the general formula of a single neural network layer.\n",
    "\n",
    "General formula:\n",
    "\n",
    "y = f(x1w1 + x2w2 + x3w3 + ... + xnwn + b)\n",
    "\n",
    "Formula in matrix form:\n",
    "\n",
    "y = f(X * W + b)\n",
    "\n",
    "where X is the input matrix, W is the matrix containing the weights, b is the bias term, and f is the non-linear function.\n",
    "\n",
    "\n",
    "#Forward Pass and Backpropagation:\n",
    "\n",
    "The process described previously is the definition of a forward pass, which means putting inputs into a neural network and obtaining an output from it.\n",
    "\n",
    "Backpropagation, on the other hand, is the process of going back works.\n",
    "\n",
    "Before we can correct our network, we need a way to measure how wrong our current model is from our target. Backpropagation uses an output, input and a \"true value\" and passes it back through the network to update its weights and biases. The \"true value\" is what the neural network should has created as output with the given input. Backpropagation uses the difference between the true value and the actual output from the network as a reference to adjust the weights and bias of its neurons. This difference is also known as the error of the neural network, which is a important benchmark to gauge the network's accuracy.\n",
    "\n",
    "There are multiple ways of calculating error, which is usually specific to what the network is designed to accomplish. This paper will use the L2 Norm function as an example, which is the following:\n",
    "\n",
    "*e(t, y) = 0.5(t - y)^2*\n",
    "*(where t is the true value and y is the actual output)*\n",
    "\n",
    "\n",
    "(Talk about derivative, break down)\n",
    "As described before, a neural network is a complicated mathamatical function at its core. As such, it is possible to obtain its derivative.\n",
    "\n",
    "\n",
    "(use dE/dw_n to find the min E value in respect to the weight w_n)\n",
    "\n",
    "Here are the functions we have so far:\n",
    "\n",
    "s = X * W + b\n",
    "\n",
    "y = f(s) = sigmoid(s)\n",
    "\n",
    "E = e(t, y) = 0.5 * (t - y)^2\n",
    "\n",
    "using chain rule:\n",
    "\n",
    "dE/dw_n = dE/dy * dy/ds * ds/dw_n\n",
    "\n",
    "ds/db_n = dE/dy * dy/ds * ds/db_n\n",
    "\n",
    "Here, we solve each required derivative:\n",
    "\n",
    "dE/dy = d/dy 0.5 * (t - y)^2 = -(t - y)\n",
    "\n",
    "dy/ds = d/ds sigmoid(s) = sigmoid(s) * (1 - sigmoid(s))\n",
    "\n",
    "ds/dw_n = d/dw_n X * W + b = d/dw_n x1w1 + x2w2 + ... + xnwn + ... + b1 + b2 + ... = d/dw_n xnwn = x_n\n",
    "\n",
    "ds/db_n = ds/db x1w1 + x2w2 + ... + b1 + b2 + ... + bn + ... = ds/db_n b_n = 1\n",
    "\n",
    "(Note, since we are finding the derivative in respect to w_n and b_n, all the other terms can be ignored since the entire function is one giant summation)\n",
    "\n",
    "Finally to combine everything:\n",
    "dE/dw_n = -(t - y) * sigmoid(s) * (1 - sigmoid(s)) * x_n\n",
    "dE/db_n = -(t - y) * sigmoid(s) * (1 - sigmoid(s)) * 1\n",
    "\n",
    "With these formulas, the direction and magnitude of how each individual neuron should be changed is known, and can be adjusted so the error function would return a minimum value.\n",
    "\n",
    "\n",
    "\n",
    "(Project result: trained XOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac099501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999993086"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Inputs\n",
    "X = np.array([2, 3, 5])\n",
    "\n",
    "#Neural Layer Properties\n",
    "W = np.array([1, 2, 4])\n",
    "B = 0\n",
    "\n",
    "#Functions\n",
    "def sigmoid(x): #Sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def error(y, t): #L2 Norm\n",
    "    return 0.5 * np.power((t-y), 2)\n",
    "\n",
    "def errorDerivative(y, t):\n",
    "    return -(t-y)\n",
    "\n",
    "class linear_neuron_layer:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sigmoid(sum(x*self.w) + self.b)\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "    \n",
    "    def updateWeights(self, g, u):\n",
    "        if len(self.w) == len(g):\n",
    "            self.w = (self.w + g) * u\n",
    "\n",
    "neuron = linear_neuron_layer(W, B)\n",
    "\n",
    "neuron.forward(X)\n",
    "#y = sigmoid(28)\n",
    "#y = 0.9999999999993086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "u = 0.7\n",
    "\n",
    "#Helpers\n",
    "def sigmoid(x): #Sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def error(y, t): #L2 Norm\n",
    "    return 0.5 * np.power((t-y), 2)\n",
    "\n",
    "def errorDerivative(y, t):\n",
    "    return -(t-y)\n",
    "\n",
    "class NeuronLayerSingle:\n",
    "    def __init__(self, w, b, f):\n",
    "        self.w = w #weight list\n",
    "        self.b = b #bias value\n",
    "        self.f = f #non-linear function\n",
    "    \n",
    "    #functions\n",
    "    def updateWeights(self, g, x, u):\n",
    "        self.w = (self.w + g*x) * u\n",
    "        self.b = self.b + g*u\n",
    "\n",
    "    def getOutputRaw(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sum(x*self.w) + self.b\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "    def getOutput(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return self.f(self.getOutputRaw(x))\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "\n",
    "class Model: #Single Layer Only\n",
    "    def __init__(self, inputSize):\n",
    "\n",
    "        mag = 1 / np.sqrt(inputSize) #shallow weight initialization\n",
    "\n",
    "        self.layer = NeuronLayerSingle(np.array([(np.random.rand() * mag * 2 - mag) for i in range(inputSize)]), np.random.rand() * mag * 2 - mag, sigmoid) #Neuron layer, init values [-1/sqrt(x), 1/sqrt(x)]\n",
    "        self.output = None\n",
    "\n",
    "    def run(self, x):\n",
    "        self.output = self.layer.getOutput(x)\n",
    "        return self.output\n",
    "    \n",
    "    def trainOnce(self, x, t, u):\n",
    "        y = self.run(x)\n",
    "        s = self.layer.getOutputRaw(x)\n",
    "        err = errorDerivative(y, t)\n",
    "        g = err * sigmoidDerivative(s)\n",
    "\n",
    "        self.layer.updateWeights(g, x, u)\n",
    "\n",
    "#For generating true value\n",
    "def actualXOR(x):\n",
    "    a = x[0]\n",
    "    b = x[1]\n",
    "    if a == b:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "trainedXOR = Model(2)\n",
    "\n",
    "for i in range(5000): #arbitrary amount of training\n",
    "    randomInput = np.array([int(np.random.rand() * 2) for i in range(2)])\n",
    "\n",
    "    trainedXOR.trainOnce(randomInput, actualXOR(randomInput), u)\n",
    "\n",
    "#Note: u values were cherry picked for better results\n",
    "#Another Note: Bias is always set to 0, so when input is [0, 0], the result would always be 0.5, since weights won't affect output\n",
    "\n",
    "print(\"-------------XOR Output\")\n",
    "\n",
    "print(trainedXOR.run(np.array([0,0]))) #0, 0.5\n",
    "print(trainedXOR.run(np.array([0,1]))) #1, 0.10408986683883949\n",
    "print(trainedXOR.run(np.array([1,0]))) #1, 0.2770619324755566\n",
    "print(trainedXOR.run(np.array([1,1]))) #0, 0.042628520033774785\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fc95f",
   "metadata": {},
   "source": [
    "#BELOW ARE UNFINISHED\n",
    "\n",
    "#Common Pitfalls and Solutions\n",
    "\n",
    "Neural Network training:\n",
    "Problems: overfitting, underfitting\n",
    "Techniques: Learning rate adjustment (optimizers), momentum(adagrad), dropout layers\n",
    "\n",
    "Convolutional Layers:\n",
    "Image processing, identify edges\n",
    "simplify into smaller tensor (embedded vectors?)\n",
    "\n",
    "Project result:\n",
    "MNIST Reader\n",
    "ResNet18 with CIFAR-10\n",
    "ViT with CIFAR-10\n",
    "\n",
    "\n",
    "Generative Programs:\n",
    "GAN\n",
    "generator vs discriminator\n",
    "\n",
    "DDRM diffusion\n",
    "image to noise and vise versa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
