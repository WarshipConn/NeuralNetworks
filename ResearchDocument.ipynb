{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2ee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#test import to make sure that the environment works\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "layer = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d36a2",
   "metadata": {},
   "source": [
    "TEST\n",
    "\n",
    "Links:\n",
    "https://saturncloud.io/blog/how-to-use-latex-in-jupyter-notebook/\n",
    "\n",
    "https://en.wikibooks.org/wiki/LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827b78d",
   "metadata": {},
   "source": [
    "Outline Very Basic:\n",
    "\n",
    "#The Main Idea behind Machine Learning\n",
    "\n",
    "It is well known that machines are potent in processing defined algorithms with its combination of speed, memory, and accuracy. Once a human defined an algorithm, or a series of steps, for the computer to follow, it can do so faster and better than any other human.\n",
    "\n",
    "However, machines themselves are unable to tackle the more abstract problems such as differentiating a photo of a dog from a cat. \n",
    "\n",
    "To humans, this task may be trivial. However, humans themselves are unable to clearly explain their thought process for separating dogs and cats in a concise way. They may suggest tips such as looking at its ears or tail, but this is another ambiguous question in itself, especially to a computer which percieves images not by its greater pattern, but by each individual pixel and its color values. For humans, their brains act like a black box: being able to intuitively process the information accurately but unsure of the exact algorithms underneath. As humans are unable to create a concise algorithm for such intuitive tasks, they cannot write code for a machine to follow in order to accomplish the same task.\n",
    "\n",
    "So, how do humans do it? Does this mean that humans were born with an innate ability to differentiate between dogs and cats? There are no strong evidence supporting this argument, so the leading theory is that humans develop their classification abilities later on, probably by observing an uncountable amount of dogs and cats throughout their lives. This implies that the classification process can be learnt, most likely by identifying groups of hidden patterns that gives deeper insight than just the raw data itself.\n",
    "\n",
    "**The broadest idea of machine learning is that there are intrinsic patterns in data. By matching and gathering a large amount input and output pairs, it may be possible to find the function or formula which converts an input into the desired corresponding output.**\n",
    "\n",
    "The rest of this paper would discuss the more practical concepts in implementing simpler neural network models.\n",
    "\n",
    "#Embedding Vectors and Representing Information (WIP):\n",
    "\n",
    "Before making a neural network, there needs to be a quantitative way of representing the information mathamatically. This is most commonly done through vectors, matrices, and tensors. These are essencially an array or list of a certain dimension. The process of converting information from one form to a vector space is known as embedding. The general idea is to map objects in the vector space based on their properties, so that more similiar items have a smaller difference. \n",
    "\n",
    "Usually, each dimension or direction in the vector space would represent a certain trait or attribute. For instance, in a good embedding of English words, the difference between vectors representing man and woman should be very similar to the difference of vectors of king and queen, boy and girl, father and mother, and so on. However in practice, larger neural networks may organize their data in another unknown method in their training.\n",
    "\n",
    "#Neurons and Linear Layers:\n",
    "\n",
    "The idea behind a neuron is that it is the smallest possible component in a larger neural network, just like a human's neuron to their brain. While biology and chemistry powers a human neuron, a machine's neuron is defined by math.\n",
    "\n",
    "In mathamatics, multiplication is the most common way to alter a value's size by its proportion. For instance, multiplying X by 0.5 yields X/2, something half as large in magnitude. Meanwhile, multiplying X by 2 yields 2X, something twice as large in magnitude. This is a useful way to amplify or diminish a value's magnitude without changing its inherent composition (attributes such as its prime factors, which may carry inherent information). For instance, if you multiply 15 by 2 to get 30, it still contain the prime factors 3 and 5 afterwards. Another simple way to manipulate values is addition. This operation can shift a value along the number line, or alternatively a vector along a certain axis. Although addition also affects the size of a value, it will disturb said value's composition. For instance, unlike previously with multiplication, if you add 15 by 2, the result 17 no longer contains the prime factor 3 and 5. Overall, it is best to think of multiplication as adjusting a value's size, while addition acts as an offset.\n",
    "\n",
    "These mathmatical ideas also applies to the field of machine learning, which comes in the form of a neuron. Instead of just being a numerical value, the input to a neuron is assumed to represent embedded information in some way unknown to us. The neuron is then able to amplify and offset the input into the final output, which is akin to adjust its significance or value. Practically, the neuron accomplish this by being a function, with two inherent adjustable properties known as the weight and the bias. The neuron will take the input signal, multiply it with its weight attribute, add the product with its bias attribute, and return the final sum as its modified output signal. For example, a neuron with a large weight would amplify the input signal into a larger output signal, and vice versa. The term for these adjustable weight and bias values is parameters.\n",
    "\n",
    "Here is a simple formula for a single neuron that incorporated the concepts from above:\n",
    "\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "\n",
    "*(where w is the weight, x is the input, and b is the bias value)*\n",
    "\n",
    "Neurons are then organized into layers, or groups of neurons in parallel. By assigning different weights to each neuron in the layers, the input signals will get amplified or diminished in its corresponding areas. Neuron layers can then be stacked sequentially, using the output of the previous layer as the input, to further add complexity and power, resulting in the final neural network.\n",
    "\n",
    "Practically, all weights, inputs, and biases are represented as matrices or tensors, both of which are a common way to group large amount of numbers. This allows for the ease of processing large amount of calculations which neural networks need. \n",
    "\n",
    "Besides the core components of weights and biases, a non-linear function is also needed to help neurons with its expressiveness. Taking a look at the current model, it is a linear function. However, not all input-output pairs can be represented by a linear model, a famous example of which is the xor logic gate. \n",
    "\n",
    "As such, the final output of a neural layer is often passed through a non-linear function before it is actually sent to the next neural layer in the model. Common examples for non-linear functions in neural networks include sigmoid and tanh.\n",
    "\n",
    "Here is a human-friendly example of a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cfb81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999993086"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inputs\n",
    "X = np.array([2, 3, 5])\n",
    "\n",
    "#Neural Layer Properties (Given)\n",
    "W = np.array([1, 2, 4])\n",
    "B = 0\n",
    "\n",
    "#Non-linear function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class linear_neuron_layer:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sigmoid(sum(x*self.w) + self.b)\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "neuron = linear_neuron_layer(W, B)\n",
    "\n",
    "neuron.forward(X)\n",
    "#y = sigmoid(28)\n",
    "#y = 0.9999999999993086"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c938e",
   "metadata": {},
   "source": [
    "After summarizing everything, here are the general formula of a single neural network layer.\n",
    "\n",
    "General formula:\n",
    "\n",
    "$$\n",
    "y = f(x_1w_1 + x_2w_2 + ... + x_nw_n + b) = f((\\sum_{i=1}^{n} x_iw_i) + b)\n",
    "$$\n",
    "\n",
    "Formula in matrix form:\n",
    "\n",
    "$$\n",
    "y = f(X*W + b)\n",
    "$$\n",
    "\n",
    "where X is the input matrix, W is the matrix containing the weights, b is the bias term, and f is the non-linear function.\n",
    "\n",
    "\n",
    "#Forward Pass and Backpropagation:\n",
    "\n",
    "The process described previously is the definition of a forward pass, which means putting inputs into a neural network and obtaining an output from it.\n",
    "\n",
    "Backpropagation, on the other hand, is the process of finding out how wrong a given model is, and then using that information to correct its weights and biases so that its accuracy is improved.\n",
    "\n",
    "Before we can correct our network, we need a way to measure how wrong our current model is from our target. Backpropagation uses an output, input and a \"true value\" and passes it back through the network to update its weights and biases. The \"true value\" is what the neural network should has created as output with the given input. Backpropagation uses the difference between the true value and the actual output from the network as a reference to adjust the weights and bias of its neurons. This difference is also known as the error of the neural network, which is a important benchmark to gauge the network's accuracy.\n",
    "\n",
    "There are multiple ways of calculating error, which is usually specific to what the network is designed to accomplish. This paper will use the L2 Norm function as an example, which is the following:\n",
    "\n",
    "$$\n",
    "e(t, y) = 0.5(t - y)^2\n",
    "$$\n",
    "*(where t is the true value and y is the actual output)*\n",
    "\n",
    "\n",
    "As described before, a neural network is a complicated mathamatical function at its core. As such, it is possible to obtain the derivatives of said functions, which in turn can be used to find the extremas (local maximums and minimums) of the neural network. Practically, the goal is to minimize the result of the error function as much as possible, and if the derivative is taken on the error function in respect to a parameter, the result will indicate how to adjust said parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfe380",
   "metadata": {},
   "source": [
    "\n",
    "Here is an example of doing backpropagation on a single linear neuron layer using the functions we have so far, which were all taken from above:\n",
    "\n",
    "Neuron layer before activation:\n",
    "\n",
    "$$\n",
    "s = X*W + b\n",
    "$$\n",
    "\n",
    "Neuron layer with sigmoid activation function:\n",
    "$$\n",
    "y = f(s) = sigmoid(s)\n",
    "$$\n",
    "\n",
    "Error of neuron layer:\n",
    "$$\n",
    "E = e(t, y) = 0.5(t - y)^2\n",
    "$$\n",
    "\n",
    "If we wish to find the derivative of the error function in respect to weight w_n and bias b_n, we can use the derivatives of the above functions and chain rule to obtain the following:\n",
    "\n",
    "$$\n",
    "\\frac{dE}{dw_n} = \\frac{dE}{dy} * \\frac{dy}{ds} * \\frac{ds}{dw_n}\n",
    "$$\n",
    "$$\n",
    "\\frac{dE}{db_n} = \\frac{dE}{dy} * \\frac{dy}{ds} * \\frac{ds}{db_n}\n",
    "$$\n",
    "\n",
    "Here, we find the derivative of each function:\n",
    "\n",
    "$$\n",
    "\\frac{dE}{dy} = \\frac{d}{dy} 0.5(t - y)^2 = -(t - y)\n",
    "$$\n",
    "$$\n",
    "\\frac{dy}{ds} = \\frac{d}{ds} sigmoid(s) = sigmoid(s) * (1 - sigmoid(s))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{ds}{dw_n} = \\frac{d}{dw_n} X * W + B = \\frac{d}{dw_n} x_1w_1 + x_2w_2 + ... + x_nw_n + ... + b_1 + b_2 + ... = \\frac{d}{dw_n} x_nw_n = x_n\n",
    "$$\n",
    "$$\n",
    "\\frac{ds}{db_n} = \\frac{d}{db_n} X * W + B = \\frac{d}{db_n} x_1w_1 + x_2w_2 + ... + b_1 + b_2 + ... + b_n + ... = \\frac{d}{db_n} db_n = x_n\n",
    "$$\n",
    "\n",
    "(Note, since we are finding the derivative in respect to w_n and b_n, all the other terms without w_n or b_n as a factor can be ignored, since the entire function is one giant summation)\n",
    "\n",
    "Here is the final result by substituting the derivative back into the overall equation:\n",
    "\n",
    "$$\n",
    "\\frac{dE}{dw_n} = -(t - y) * sigmoid(s) * (1 - sigmoid(s)) * x_n\n",
    "$$\n",
    "$$\n",
    "\\frac{dE}{db_n} = -(t - y) * sigmoid(s) * (1 - sigmoid(s))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4b7e0",
   "metadata": {},
   "source": [
    "\n",
    "With these formulas, the direction and magnitude of how each individual neuron should be changed is known, and can be adjusted so the error function would return a lower value. The collection of derivates in respect to every parameter is known as the gradient.\n",
    "\n",
    "There is one caveat regarding backpropagation. Since the derivatives were found assuming all other variables are constant, the final model after each parameters were tweaked may not reflect a perfectly downward trend in error, as all of the weights or biases would have been shifted slightly. This problem is reduced by multiplying the gradient by a value called the learning rate, which is a small constant used to reduce the changes on the model. In other words, learning rate reduces the magnitude of change to the model in order to allow it to adjust in more precise steps.\n",
    "\n",
    "Here is the Python implementation of backpropagation onto the linear_neuron_layer class from before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac099501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "X = np.array([2, 3, 5])\n",
    "\n",
    "#Neural Layer Properties\n",
    "W = np.array([1, 2, 4])\n",
    "B = 0\n",
    "\n",
    "#Functions\n",
    "def sigmoid(x): #Sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def error(y, t): #L2 Norm\n",
    "    return 0.5 * np.power((t-y), 2)\n",
    "\n",
    "def errorDerivative(y, t):\n",
    "    return -(t-y)\n",
    "\n",
    "class linear_neuron_layer:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def S(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sum(x*self.w) + self.b\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sigmoid(self.S(x))\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "    \n",
    "    def updateWeights(self, g, u): #g is gradient, u is learning rate\n",
    "        if len(self.w) == len(g):\n",
    "            self.w = self.w + g*self.w * u\n",
    "            self.b = self.b + g*u\n",
    "    \n",
    "    def backpropagate(self, x, t, u):\n",
    "        y = self.forward(x)\n",
    "        s = self.S(x)\n",
    "        err = errorDerivative(y, t)\n",
    "        g = err * sigmoidDerivative(s)\n",
    "\n",
    "        self.updateWeights(g, x, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6693c8",
   "metadata": {},
   "source": [
    "AND project below\n",
    "\n",
    "#Project Example: AND Gate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9d054e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-0.38720493  0.37201764] | bias: 0.5959288497049258\n",
      "-------------AND Before Training\n",
      "0.20783473719069062\n",
      "0.1523477827507228\n",
      "0.2626022918709611\n",
      "0.06435507895044915\n",
      "average error: 0.17178497269070592\n",
      "weights: [0.10292532 0.71379843] | bias: 2.1524315246499093\n",
      "-------------AND After Training\n",
      "0.4013146199783163\n",
      "0.409613508764064\n",
      "0.4476014172304342\n",
      "0.001192621449730052\n",
      "average error: 0.3149305418556361\n",
      "-------------AND Answer\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-------------AND Output\n",
      "0.8958957751639599\n",
      "0.9461515916917692\n",
      "0.9051116050124028\n",
      "0.9511610514091458\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "u = 0.01\n",
    "\n",
    "#Helpers\n",
    "def sigmoid(x): #Sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def error(y, t): #L2 Norm\n",
    "    return 0.5 * np.power((t-y), 2)\n",
    "\n",
    "def errorDerivative(y, t):\n",
    "    return -(t-y)\n",
    "\n",
    "class NeuronLayerSingle:\n",
    "    def __init__(self, w, b, f):\n",
    "        self.w = w #weight list\n",
    "        self.b = b #bias value\n",
    "        self.f = f #non-linear function\n",
    "    \n",
    "    #functions\n",
    "            \n",
    "    def updateWeights(self, g, x, u):\n",
    "        self.w = self.w + g*x*u\n",
    "        self.b = self.b + g*u\n",
    "\n",
    "    def getOutputRaw(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return sum(x*self.w) + self.b\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "\n",
    "    def getOutput(self, x):\n",
    "        if len(x) == len(self.w):\n",
    "            return self.f(self.getOutputRaw(x))\n",
    "        else:\n",
    "            return \"Weight/Input Mismatch\"\n",
    "    \n",
    "    def toString(self):\n",
    "        return f\"weights: {self.w} | bias: {self.b}\"\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, inputSize):\n",
    "\n",
    "        mag = 1 / np.sqrt(inputSize) #shallow weight initialization\n",
    "\n",
    "        #Neuron layer, init values [-1/sqrt(x), 1/sqrt(x)]\n",
    "\n",
    "        self.layer1 = NeuronLayerSingle(w=np.array([(np.random.rand() * mag * 2 - mag) for i in range(inputSize)]),\n",
    "                                       b=np.random.rand() * mag * 2 - mag, \n",
    "                                       f=sigmoid\n",
    "                                       )\n",
    "\n",
    "        self.output = None\n",
    "\n",
    "    def run(self, x):\n",
    "        self.output = self.layer1.getOutput(x)\n",
    "        return self.output\n",
    "    \n",
    "    def getError(self, x, t):\n",
    "        y = self.run(x)\n",
    "        err = error(y, t)\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def trainOnce(self, x, t, u):\n",
    "        y = self.run(x)\n",
    "        err = errorDerivative(y, t)\n",
    "\n",
    "        x1 = x\n",
    "        s1 = self.layer1.getOutputRaw(x1)\n",
    "        g1 = err * sigmoidDerivative(s1)\n",
    "\n",
    "        self.layer1.updateWeights(g1, x1, u)\n",
    "\n",
    "    def toString(self):\n",
    "        print(self.layer1.toString())\n",
    "\n",
    "        \n",
    "\n",
    "#For generating true value\n",
    "def actualAND(x):\n",
    "    a = x[0]\n",
    "    b = x[1]\n",
    "\n",
    "    if a == 1 and a == b:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "trainedAND = Model(2)\n",
    "\n",
    "training_data = [\n",
    "    np.array([0, 0]),\n",
    "    np.array([1, 0]),\n",
    "    np.array([0, 1]),\n",
    "    np.array([1, 1])\n",
    "]\n",
    "\n",
    "trainedAND.toString()\n",
    "print(\"-------------AND Before Training\")\n",
    "\n",
    "avg_error = 0\n",
    "for input in training_data:\n",
    "    cur_error = trainedAND.getError(input, actualAND(input))\n",
    "    avg_error += cur_error\n",
    "    print(cur_error)\n",
    "avg_error /= 4\n",
    "print(f\"average error: {avg_error}\")\n",
    "\n",
    "\n",
    "for i in range(500): #arbitrary amount of training epochs\n",
    "    for training_input in training_data:\n",
    "        trainedAND.trainOnce(training_input, actualAND(training_input), u)\n",
    "\n",
    "#Note: u values were cherry picked for better results\n",
    "\n",
    "trainedAND.toString()\n",
    "print(\"-------------AND After Training\")\n",
    "avg_error = 0\n",
    "for input in training_data:\n",
    "    cur_error = trainedAND.getError(input, actualAND(input))\n",
    "    avg_error += cur_error\n",
    "    print(cur_error)\n",
    "avg_error /= 4\n",
    "print(f\"average error: {avg_error}\")\n",
    "\n",
    "print(\"-------------AND Answer\")\n",
    "\n",
    "print(actualAND(np.array([0,0])))\n",
    "print(actualAND(np.array([0,1])))\n",
    "print(actualAND(np.array([1,0])))\n",
    "print(actualAND(np.array([1,1])))\n",
    "\n",
    "print(\"-------------AND Output\")\n",
    "\n",
    "print(trainedAND.run(np.array([0,0])))\n",
    "print(trainedAND.run(np.array([0,1])))\n",
    "print(trainedAND.run(np.array([1,0])))\n",
    "print(trainedAND.run(np.array([1,1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fc95f",
   "metadata": {},
   "source": [
    "#BELOW ARE UNFINISHED\n",
    "\n",
    "#Common Pitfalls and Solutions\n",
    "\n",
    "After discussing the theoretical of training a neural network, this section will discuss the more practical issues with Neural Network trainings, especially common problems that may arise during backpropagation.\n",
    "\n",
    "One of the most important aspect in training is ensuring the quality and the quantity of training data, since it is what the model would base its behavior and patterns on. It is recommended to check for unwanted noise, clarity, erronous \"true\" values, and the normalization for each input and answer pair. Overall, it is good practice to prevent the \"garbage in, garbage out\" situation (where bad inputs naturally leads to bad results).\n",
    "\n",
    "Underfitting is an issue which occurs when a neural network is too simple or small to effectively do its task, or that the neural network did not recieve adequate training.\n",
    "Symptoms of this issue include seemingly random outputs or high skewness. The best solutions are to either increase the complexity of the model by adding more layers, or to run additional epochs on the model.\n",
    "\n",
    "On the otherhand, overfitting is the problem where a model is trained with the same constant set of training data for too much, leading to inflexibility against new, unseen data. In a more human metaphor, overfitting is akin to reciting answers to every question, rather than learning to solve them. Although this might make them excel at the original training data, they are practically useless, as their ultimate final goal was to help identify new values, not to classify known values. One great solution is known as dropoff layer. Unlike other neural network layers, dropoff layers is a simple utility layer which randomly removes parts of its input, before passing the rest onwards to the rest of the model. This helps prevent the model from overfixating or overelying on a single data point, and ensure that it is robust enouogh to withstand more interference, and thereby remain flexible. Alternatively, simply running less epochs of training may help the model stay flexible, and less fixated on the given training data.\n",
    "\n",
    "\n",
    "\n",
    "Neural Network training:\n",
    "Problems: overfitting, underfitting\n",
    "Techniques: Learning rate adjustment (optimizers), momentum(adagrad), dropout layers\n",
    "\n",
    "\n",
    "#Convolutions and Image Processing\n",
    "\n",
    "Convolution is a common way to aggregate data. \n",
    "\n",
    "In the context of image processing, convolution uses a kernel (a tensor of numbers) and multiply each internal value with a respective value taken from a section of the input tensor. These products are then added together to return a constant as the final result of the convolution operation. \n",
    "\n",
    "(Add diagram here)\n",
    "\n",
    "The reasoning behind using convolutions in image processing is that it can summarize sections of pixels at once. Usually, each individual pixel by itself holds little significant, but by considering multiple of them together, more information can be extracted. In other words, the whole is greater than the sum of its parts. \n",
    "\n",
    "Even if a linear model may theoretically work with infinite time and training, convolutional layers help bring things back into the practical realm by simplfying large tensors into smaller, informationally-dense counterparts. This effectively embeds the picture into a high dimentional vector plane, before transforming it into the desired output form through more neural layers.\n",
    "\n",
    "\n",
    "\n",
    "Project result:\n",
    "MNIST Reader\n",
    "ResNet18 with CIFAR-10\n",
    "ViT with CIFAR-10\n",
    "\n",
    "\n",
    "Generative Programs:\n",
    "GAN\n",
    "generator vs discriminator\n",
    "\n",
    "DDRM diffusion\n",
    "image to noise and vise versa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
